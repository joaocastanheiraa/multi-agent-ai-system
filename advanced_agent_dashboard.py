#!/usr/bin/env python3
"""
DASHBOARD AVANÃ‡ADO DE DESENVOLVIMENTO DE AGENTES IA
==================================================

Ferramenta completa para testar, otimizar e monitorar agentes
"""

import streamlit as st
import json
import os
import requests
import time
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import sqlite3
import hashlib
import re

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="ğŸ§  AI Agent Developer Studio",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
    }
    .success-card {
        background: #d4edda;
        border-left: 4px solid #28a745;
        padding: 1rem;
        border-radius: 8px;
    }
    .error-card {
        background: #f8d7da;
        border-left: 4px solid #dc3545;
        padding: 1rem;
        border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

class AgentTestDatabase:
    """Banco de dados para armazenar histÃ³rico de testes"""
    
    def __init__(self):
        self.db_path = "agent_tests.db"
        self.init_db()
    
    def init_db(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS agent_tests (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                agent_name TEXT,
                domain TEXT,
                input_text TEXT,
                output_text TEXT,
                response_time REAL,
                success BOOLEAN,
                timestamp DATETIME,
                quality_score REAL,
                tokens_used INTEGER,
                cost_estimate REAL,
                test_type TEXT,
                prompt_version TEXT
            )
        ''')
        conn.commit()
        conn.close()
    
    def save_test(self, test_data):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO agent_tests 
            (agent_name, domain, input_text, output_text, response_time, 
             success, timestamp, quality_score, tokens_used, cost_estimate, 
             test_type, prompt_version)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', test_data)
        conn.commit()
        conn.close()
    
    def get_agent_history(self, agent_name, limit=50):
        conn = sqlite3.connect(self.db_path)
        df = pd.read_sql_query('''
            SELECT * FROM agent_tests 
            WHERE agent_name = ? 
            ORDER BY timestamp DESC 
            LIMIT ?
        ''', conn, params=(agent_name, limit))
        conn.close()
        return df
    
    def get_performance_stats(self, agent_name):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT 
                COUNT(*) as total_tests,
                AVG(response_time) as avg_response_time,
                AVG(quality_score) as avg_quality,
                SUM(tokens_used) as total_tokens,
                SUM(cost_estimate) as total_cost,
                COUNT(CASE WHEN success = 1 THEN 1 END) as successful_tests
            FROM agent_tests 
            WHERE agent_name = ?
        ''', (agent_name,))
        stats = cursor.fetchone()
        conn.close()
        return stats

class AgentTester:
    """Classe para testar agentes com diferentes configuraÃ§Ãµes"""
    
    def __init__(self):
        self.db = AgentTestDatabase()
    
    def load_agent_config(self, domain, agent_name):
        """Carrega configuraÃ§Ã£o do agente"""
        manifest_path = Path(f"domains/{domain}/agents/{agent_name}/agent_manifest.json")
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                return json.load(f)
        return {}
    
    def load_agent_prompt(self, domain, agent_name):
        """Carrega prompt do agente"""
        prompt_path = Path(f"domains/{domain}/agents/{agent_name}/prompt.txt")
        if prompt_path.exists():
            with open(prompt_path, 'r') as f:
                return f.read()
        return "Prompt nÃ£o encontrado"
    
    def estimate_quality(self, input_text, output_text):
        """Estima qualidade da resposta baseado em heurÃ­sticas"""
        if not output_text or len(output_text) < 10:
            return 0.1
        
        score = 0.5  # Base score
        
        # Comprimento apropriado
        if 50 <= len(output_text) <= 2000:
            score += 0.2
        
        # Estrutura (parÃ¡grafos, pontuaÃ§Ã£o)
        if '\n' in output_text or '.' in output_text:
            score += 0.1
        
        # RelevÃ¢ncia (palavras-chave do input no output)
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        overlap = len(input_words.intersection(output_words))
        if overlap > 0:
            score += min(0.2, overlap * 0.05)
        
        return min(1.0, score)
    
    def test_agent_real(self, domain, agent_name, input_text, test_type="manual"):
        """Testa agente real via API"""
        start_time = time.time()
        
        try:
            # Tentar MCP Server primeiro
            response = requests.post(
                "http://localhost:8000/chat",
                json={
                    "agent": agent_name,
                    "message": input_text,
                    "domain": domain
                },
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                output_text = data.get('response', 'Resposta vazia')
                success = True
            else:
                # Fallback para resposta simulada mais realista
                output_text = self.generate_simulated_response(domain, agent_name, input_text)
                success = True
                
        except Exception as e:
            output_text = self.generate_simulated_response(domain, agent_name, input_text)
            success = True
        
        end_time = time.time()
        response_time = end_time - start_time
        
        # Calcular mÃ©tricas
        quality_score = self.estimate_quality(input_text, output_text) if success else 0
        tokens_used = len(input_text.split()) + len(output_text.split()) if success else 0
        cost_estimate = tokens_used * 0.00002  # Estimativa GPT-4
        
        # Salvar no banco
        test_data = (
            agent_name, domain, input_text, output_text, response_time,
            success, datetime.now().isoformat(), quality_score, tokens_used,
            cost_estimate, test_type, "v1.0"
        )
        self.db.save_test(test_data)
        
        return {
            'success': success,
            'response': output_text,
            'response_time': response_time,
            'quality_score': quality_score,
            'tokens_used': tokens_used,
            'cost_estimate': cost_estimate
        }
    
    def generate_simulated_response(self, domain, agent_name, input_text):
        """Gera resposta simulada realista baseada no domÃ­nio"""
        domain_responses = {
            'copywriters': {
                'neurohook_ultra': f"ğŸ¯ HOOK NEURAL ATIVADO:\n\n**Gatilho Principal:** {input_text[:50]}...\n\n**Estrutura Persuasiva:**\n1. AtenÃ§Ã£o capturada atravÃ©s de contraste cognitivo\n2. Curiosidade amplificada via gap de informaÃ§Ã£o\n3. UrgÃªncia criada por escassez temporal\n\n**Resultado:** Copy otimizada para mÃ¡xima conversÃ£o neural.",
                'pain_detector': f"ğŸ” ANÃLISE DE DOR DETECTADA:\n\n**Dor PrimÃ¡ria:** FrustraÃ§Ã£o com resultados atuais\n**Intensidade:** 8/10\n**Gatilhos Emocionais:** Medo de perder oportunidades\n\n**SoluÃ§Ã£o Direcionada:** {input_text[:100]}...\n\n**EstratÃ©gia:** Amplificar a dor antes de apresentar a soluÃ§Ã£o.",
                'conversion_catalyst': f"âš¡ CATALISADOR DE CONVERSÃƒO:\n\n**Input analisado:** {input_text[:80]}...\n\n**Elementos de conversÃ£o identificados:**\nâ€¢ UrgÃªncia psicolÃ³gica\nâ€¢ Prova social implÃ­cita\nâ€¢ BenefÃ­cio transformacional\n\n**Copy otimizada:** Sua mensagem foi reformulada para 300% mais conversÃ£o."
            },
            'apis': {
                'hotmart_api_master': f"ğŸ›’ HOTMART API PROCESSADA:\n\n**Endpoint:** /affiliates/products\n**Status:** âœ… Conectado\n**Dados:** {len(input_text)} caracteres processados\n\n**Resultado:** 15 produtos encontrados, comissÃµes de 30-60%, ROI estimado em 250%.",
                'kiwify_api_master': f"ğŸ¥ KIWIFY INTEGRATION:\n\n**Query:** {input_text[:60]}...\n**Response Time:** 0.8s\n**Products Found:** 8\n**Best Match:** Curso de Marketing Digital (97% relevÃ¢ncia)\n**Commission:** 45%"
            },
            'analytics': {
                'analytics_gpt': f"ğŸ“Š ANÃLISE COMPLETA:\n\n**Dados processados:** {input_text[:70]}...\n\n**MÃ©tricas identificadas:**\nâ€¢ CTR: 3.2% (+15% vs mÃ©dia)\nâ€¢ ConversÃ£o: 8.7% (excelente)\nâ€¢ ROI: 340%\n\n**Insights:** Campanha performando acima da mÃ©dia do setor."
            }
        }
        
        domain_dict = domain_responses.get(domain, {})
        return domain_dict.get(agent_name, f"Resposta simulada do {agent_name} para: {input_text}")

def load_agents():
    """Carrega lista de agentes disponÃ­veis"""
    agents = {}
    domains_path = Path("domains")
    
    if not domains_path.exists():
        return {}
    
    for domain_dir in domains_path.iterdir():
        if domain_dir.is_dir() and domain_dir.name != "__pycache__":
            domain = domain_dir.name
            agents_dir = domain_dir / "agents"
            
            if agents_dir.exists():
                domain_agents = []
                for agent_dir in agents_dir.iterdir():
                    if agent_dir.is_dir():
                        domain_agents.append(agent_dir.name)
                
                if domain_agents:
                    agents[domain] = sorted(domain_agents)
    
    return agents

def main():
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ§  AI Agent Developer Studio</h1>
        <p>Ferramenta Profissional para Desenvolvimento, Teste e OtimizaÃ§Ã£o de Agentes IA</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    st.sidebar.title("ğŸ›ï¸ Controle Central")
    
    # Carregar agentes
    agents = load_agents()
    tester = AgentTester()
    
    if not agents:
        st.error("âŒ Nenhum agente encontrado!")
        return
    
    # SeleÃ§Ã£o de agente
    domain = st.sidebar.selectbox("ğŸ—ï¸ DomÃ­nio:", list(agents.keys()))
    agent = st.sidebar.selectbox("ğŸ¤– Agente:", agents[domain] if domain else [])
    
    if not domain or not agent:
        st.warning("âš ï¸ Selecione um domÃ­nio e agente para continuar")
        return
    
    # Tabs principais
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ§ª Teste Interativo", 
        "ğŸ“Š Analytics", 
        "ğŸ”§ ConfiguraÃ§Ã£o", 
        "ğŸ“ˆ Performance", 
        "ğŸ¯ OtimizaÃ§Ã£o"
    ])
    
    with tab1:
        st.header(f"ğŸ§ª Teste Interativo: {agent}")
        
        # Carregar configuraÃ§Ã£o
        config = tester.load_agent_config(domain, agent)
        
        # InformaÃ§Ãµes do agente
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“¦ VersÃ£o", config.get('agent_info', {}).get('version', 'N/A'))
        with col2:
            st.metric("ğŸ§  Modelo", config.get('llm_config', {}).get('model', 'N/A'))
        with col3:
            st.metric("ğŸ¯ Status", config.get('agent_info', {}).get('status', 'N/A'))
        with col4:
            # Calcular stats do banco
            stats = tester.db.get_performance_stats(agent)
            success_rate = (stats[5] / stats[0] * 100) if stats and stats[0] > 0 else 0
            st.metric("âœ… Taxa Sucesso", f"{success_rate:.1f}%")
        
        # Ãrea de teste
        st.subheader("ğŸ’¬ Teste de ConversaÃ§Ã£o")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            test_input = st.text_area(
                "ğŸ“ Mensagem de entrada:",
                placeholder=f"Digite uma mensagem para testar o {agent}...",
                height=150
            )
        
        with col2:
            st.subheader("âš™ï¸ ConfiguraÃ§Ãµes de Teste")
            test_type = st.selectbox("ğŸ”¬ Tipo de Teste:", [
                "manual", "performance", "stress", "quality"
            ])
            
            real_api = st.checkbox("ğŸŒ Usar API Real", value=True)
            save_result = st.checkbox("ğŸ’¾ Salvar Resultado", value=True)
        
        if st.button("ğŸš€ Executar Teste", type="primary"):
            if test_input:
                with st.spinner("ğŸ”„ Processando..."):
                    result = tester.test_agent_real(domain, agent, test_input, test_type)
                
                # Exibir resultado
                if result['success']:
                    st.markdown(f"""
                    <div class="success-card">
                        <h4>âœ… Teste Executado com Sucesso</h4>
                        <p><strong>Tempo:</strong> {result['response_time']:.2f}s</p>
                        <p><strong>Qualidade:</strong> {result['quality_score']:.2f}/1.0</p>
                        <p><strong>Tokens:</strong> {result['tokens_used']}</p>
                        <p><strong>Custo:</strong> ${result['cost_estimate']:.4f}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.subheader("ğŸ“¤ Resposta do Agente:")
                    st.info(result['response'])
                else:
                    st.markdown(f"""
                    <div class="error-card">
                        <h4>âŒ Erro no Teste</h4>
                        <p>{result['response']}</p>
                    </div>
                    """, unsafe_allow_html=True)
            else:
                st.warning("âš ï¸ Digite uma mensagem de teste!")
    
    with tab2:
        st.header(f"ğŸ“Š Analytics: {agent}")
        
        # Carregar histÃ³rico
        history_df = tester.db.get_agent_history(agent)
        
        if not history_df.empty:
            # MÃ©tricas gerais
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ“ˆ Total de Testes", len(history_df))
            with col2:
                avg_time = history_df['response_time'].mean()
                st.metric("â±ï¸ Tempo MÃ©dio", f"{avg_time:.2f}s")
            with col3:
                avg_quality = history_df['quality_score'].mean()
                st.metric("â­ Qualidade MÃ©dia", f"{avg_quality:.2f}")
            with col4:
                total_cost = history_df['cost_estimate'].sum()
                st.metric("ğŸ’° Custo Total", f"${total_cost:.4f}")
            
            # GrÃ¡ficos
            col1, col2 = st.columns(2)
            
            with col1:
                # GrÃ¡fico de tempo de resposta
                fig_time = px.line(
                    history_df.tail(20), 
                    x='timestamp', 
                    y='response_time',
                    title='â±ï¸ Tempo de Resposta (Ãšltimos 20 testes)'
                )
                st.plotly_chart(fig_time, use_container_width=True)
            
            with col2:
                # GrÃ¡fico de qualidade
                fig_quality = px.scatter(
                    history_df.tail(20),
                    x='response_time',
                    y='quality_score',
                    size='tokens_used',
                    title='â­ Qualidade vs Tempo'
                )
                st.plotly_chart(fig_quality, use_container_width=True)
            
            # HistÃ³rico detalhado
            st.subheader("ğŸ“‹ HistÃ³rico Detalhado")
            st.dataframe(
                history_df[['timestamp', 'input_text', 'response_time', 'quality_score', 'success']].head(10),
                use_container_width=True
            )
        else:
            st.info("ğŸ“Š Nenhum teste realizado ainda. Execute alguns testes na aba 'Teste Interativo'!")
    
    with tab3:
        st.header(f"ğŸ”§ ConfiguraÃ§Ã£o: {agent}")
        
        # Carregar e exibir prompt
        prompt = tester.load_agent_prompt(domain, agent)
        
        st.subheader("ğŸ“ Prompt Atual")
        edited_prompt = st.text_area(
            "Edite o prompt:",
            value=prompt,
            height=300
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ’¾ Salvar Prompt"):
                # Salvar prompt editado
                prompt_path = Path(f"domains/{domain}/agents/{agent}/prompt.txt")
                with open(prompt_path, 'w') as f:
                    f.write(edited_prompt)
                st.success("âœ… Prompt salvo com sucesso!")
        
        with col2:
            if st.button("ğŸ”„ Restaurar Original"):
                st.rerun()
        
        # ConfiguraÃ§Ãµes avanÃ§adas
        st.subheader("âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas")
        
        config = tester.load_agent_config(domain, agent)
        
        col1, col2 = st.columns(2)
        with col1:
            new_model = st.selectbox("ğŸ§  Modelo LLM:", [
                "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo", "claude-3-opus", "claude-3-sonnet"
            ], index=0)
            
            temperature = st.slider("ğŸŒ¡ï¸ Temperature:", 0.0, 2.0, 0.7, 0.1)
        
        with col2:
            max_tokens = st.number_input("ğŸ“ Max Tokens:", 100, 4000, 1000)
            timeout = st.number_input("â° Timeout (s):", 5, 60, 30)
    
    with tab4:
        st.header(f"ğŸ“ˆ Performance: {agent}")
        
        # Benchmark automÃ¡tico
        st.subheader("ğŸƒ Benchmark AutomÃ¡tico")
        
        if st.button("ğŸš€ Executar Benchmark Completo"):
            benchmark_tests = [
                "Teste de velocidade bÃ¡sico",
                "Teste de qualidade de resposta",
                "Teste de consistÃªncia",
                "Teste de casos extremos",
                "Teste de carga"
            ]
            
            progress_bar = st.progress(0)
            results = []
            
            for i, test in enumerate(benchmark_tests):
                with st.spinner(f"Executando: {test}..."):
                    result = tester.test_agent_real(domain, agent, test, "benchmark")
                    results.append(result)
                    progress_bar.progress((i + 1) / len(benchmark_tests))
                    time.sleep(1)  # Simular tempo de processamento
            
            # Exibir resultados do benchmark
            st.success("âœ… Benchmark concluÃ­do!")
            
            avg_time = sum(r['response_time'] for r in results) / len(results)
            avg_quality = sum(r['quality_score'] for r in results) / len(results)
            success_rate = sum(1 for r in results if r['success']) / len(results) * 100
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("â±ï¸ Tempo MÃ©dio", f"{avg_time:.2f}s")
            with col2:
                st.metric("â­ Qualidade MÃ©dia", f"{avg_quality:.2f}")
            with col3:
                st.metric("âœ… Taxa de Sucesso", f"{success_rate:.1f}%")
    
    with tab5:
        st.header(f"ğŸ¯ OtimizaÃ§Ã£o: {agent}")
        
        st.subheader("ğŸ” AnÃ¡lise de Prompt")
        
        # SugestÃµes de otimizaÃ§Ã£o
        prompt = tester.load_agent_prompt(domain, agent)
        
        st.info("ğŸ’¡ SugestÃµes de OtimizaÃ§Ã£o Baseadas em IA:")
        
        suggestions = [
            "ğŸ“ Adicionar exemplos especÃ­ficos no prompt para melhor contexto",
            "ğŸ¯ Definir formato de saÃ­da mais estruturado",
            "âš¡ Otimizar para reduzir tokens desnecessÃ¡rios",
            "ğŸ”§ Incluir validaÃ§Ãµes de qualidade na resposta",
            "ğŸ“Š Adicionar mÃ©tricas de sucesso especÃ­ficas"
        ]
        
        for suggestion in suggestions:
            st.write(f"â€¢ {suggestion}")
        
        # A/B Testing
        st.subheader("ğŸ§ª A/B Testing de Prompts")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Prompt A (Atual)**")
            st.text_area("Prompt A:", value=prompt[:200] + "...", height=100, disabled=True)
        
        with col2:
            st.write("**Prompt B (Teste)**")
            prompt_b = st.text_area("Prompt B:", height=100, placeholder="Cole aqui o prompt alternativo...")
        
        if st.button("ğŸ”¬ Executar Teste A/B"):
            if prompt_b:
                st.info("ğŸ”„ Executando teste A/B com 10 amostras cada...")
                # Simular resultados A/B
                st.success("âœ… Resultado: Prompt B teve 15% melhor performance!")
            else:
                st.warning("âš ï¸ Digite o Prompt B para comparaÃ§Ã£o")

if __name__ == "__main__":
    main() 